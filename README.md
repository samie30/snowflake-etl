
# 🚀 S3 to Snowflake ETL Pipeline (Trainee Project)

This project showcases how to create an end-to-end ETL pipeline from AWS S3 to Snowflake using Python. As a **trainee and fresher**, I built this to demonstrate hands-on skills with cloud services, database integration, and automation.

## 🔍 Features
- S3 Bucket creation and data upload using `boto3`
- Snowflake table creation and data load via external stage
- Python-based orchestration
- Environment-based secure credential management

## 🧰 Tech Stack
- AWS S3
- Snowflake
- Python 3.10
- Boto3
- Snowflake Connector
- Dotenv

## ⚙️ Steps to Reproduce
1. Clone this repo
2. Fill in `.env` with your credentials
3. Run `python scripts/etl_pipeline.py`

## 📈 Output
The pipeline loads a CSV with employee records into a Snowflake table. This demonstrates a full data flow from cloud storage to data warehouse.

## 📌 Why This Project Stands Out
- Built completely as a **self-initiated learning project**.
- Uses best practices like `.env`, modular code, and automation.
- Clearly shows readiness for **real-world cloud data engineering workflows**.

---
**🔗 GitHub:** [github.com/yourusername/s3-snowflake-etl](https://github.com/yourusername/s3-snowflake-etl)

**⭐ Star this repo if you like it!**
